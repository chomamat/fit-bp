{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 565
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "b3FAMmp61N9O",
    "outputId": "9189cf43-c9d8-47d1-d941-0931e83ad8c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch==1.0.0 from https://download.pytorch.org/whl/cu80/torch-1.0.0-cp36-cp36m-linux_x86_64.whl in /usr/local/lib/python3.6/dist-packages (1.0.0)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (0.2.2.post3)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.0.0)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.11.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.14.6)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision) (4.1.1)\n",
      "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from pillow>=4.1.1->torchvision) (0.46)\n",
      "Requirement already satisfied: torchsummary in /usr/local/lib/python3.6/dist-packages (1.5.1)\n",
      "Collecting tensorboardX\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5c/76/89dd44458eb976347e5a6e75eb79fecf8facd46c1ce259bad54e0044ea35/tensorboardX-1.6-py2.py3-none-any.whl (129kB)\n",
      "\u001b[K    100% |████████████████████████████████| 133kB 4.7MB/s \n",
      "\u001b[?25hRequirement already satisfied: protobuf>=3.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (3.7.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.14.6)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.11.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.2.0->tensorboardX) (40.8.0)\n",
      "Installing collected packages: tensorboardX\n",
      "Successfully installed tensorboardX-1.6\n",
      "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
      "total 949904\n",
      "drwxr-xr-x 3 root root      4096 Mar 28 10:34 data\n",
      "-rw------- 1 root root      2875 Mar 28 15:24 _dataset_tools.py\n",
      "drwx------ 3 root root      4096 Mar 28 11:07 gdrive\n",
      "-rw------- 1 root root      2481 Mar 28 15:24 _my_tools.py\n",
      "drwxr-xr-x 2 root root      4096 Mar 28 15:19 __pycache__\n",
      "drwxr-xr-x 1 root root      4096 Mar  8 17:26 sample_data\n",
      "-rw------- 1 root root 129687680 Mar 28 15:23 X_test.npy\n",
      "-rw------- 1 root root 518750336 Mar 28 15:24 X_train.npy\n",
      "-rw------- 1 root root  64843904 Mar 28 15:24 y_test.npy\n",
      "-rw------- 1 root root 259375232 Mar 28 15:24 y_train.npy\n"
     ]
    }
   ],
   "source": [
    "!pip3 install https://download.pytorch.org/whl/cu80/torch-1.0.0-cp36-cp36m-linux_x86_64.whl\n",
    "!pip3 install torchvision\n",
    "!pip3 install torchsummary\n",
    "!pip3 install tensorboardX\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')\n",
    "!cp gdrive/My\\ Drive/x64/*.npy .\n",
    "!cp gdrive/My\\ Drive/*.py .\n",
    "!ls -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "QG3xCwCI1Mfg",
    "outputId": "79968c1c-9f99-4397-a609-12341c5e9b6e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device is cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# from tensorboardX import SummaryWriter\n",
    "from torchsummary import summary\n",
    "\n",
    "import _my_tools as mt\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device is\", device)\n",
    "X_train, y_train, X_test, y_test = mt.loadData(\"dataset_interpolation/\",'float16',channels_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ScRT-TqfAAyg",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        self.KERNEL_SIZE = 31\n",
    "        self.activation = nn.ReLU()\n",
    "        \n",
    "        self.conv_setup = {\n",
    "            'kernel' : (3,3),\n",
    "            'stride' : (1,1),\n",
    "            'padding' : 1,\n",
    "            'activation' : self.activation\n",
    "        }\n",
    "        self.pooling_setup = {\n",
    "            'kernel_size' : (2,2),\n",
    "            'stride' : (2,2)\n",
    "        }\n",
    "        self.upsample_setup = {\n",
    "            'scale_factor' : 2,\n",
    "            'mode' : 'bilinear',\n",
    "            'align_corners' : True\n",
    "        }\n",
    "\n",
    "        self.pooling_layer = nn.AvgPool2d(**self.pooling_setup)\n",
    "        self.upsample_layer = nn.Upsample(**self.upsample_setup)\n",
    "        \n",
    "        self.conv32 = self._convBlock(2, 32, **self.conv_setup)\n",
    "        self.conv64 = self._convBlock(32, 64, **self.conv_setup)\n",
    "        self.conv128 = self._convBlock(64, 128, **self.conv_setup)\n",
    "        self.conv256 = self._convBlock(128, 256, **self.conv_setup)\n",
    "        self.conv256_256 = self._convBlock(256, 256, **self.conv_setup)\n",
    "\n",
    "\n",
    "        self.upsample256 = self._upsampleBlock(self.upsample_layer, 256, 256, **self.conv_setup)\n",
    "        self.deconv128 = self._convBlock(256, 128, **self.conv_setup)\n",
    "        self.upsample128 = self._upsampleBlock(self.upsample_layer, 128, 128, **self.conv_setup)\n",
    "        self.deconv64 = self._convBlock(128, 64, **self.conv_setup)\n",
    "        self.upsample64 = self._upsampleBlock(self.upsample_layer, 64, 64, **self.conv_setup)\n",
    "        self.deconv32 = self._convBlock(64, 32, **self.conv_setup)\n",
    "        self.upsample32 = self._upsampleBlock(self.upsample_layer, 32, 32, **self.conv_setup)\n",
    "        self.deconv1 = self._convBlock(32, 1, **self.conv_setup)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x32 = self.conv32(x)\n",
    "        x32_p = self.pooling_layer(x32)\n",
    "        x64 = self.conv64(x32_p)\n",
    "        x64_p = self.pooling_layer(x64)\n",
    "        x128 = self.conv128(x64_p)\n",
    "        x128_p = self.pooling_layer(x128)\n",
    "        x256 = self.conv256(x128_p)\n",
    "        x256_p = self.pooling_layer(x256)\n",
    "\n",
    "        x = self.conv256_256(x256_p)\n",
    "\n",
    "        # expansion\n",
    "\n",
    "        x = self.upsample256(x)\n",
    "        x += x256\n",
    "        x = self.deconv128(x)\n",
    "\n",
    "        x = self.upsample128(x)\n",
    "        x += x128\n",
    "        x = self.deconv64(x)\n",
    "\n",
    "        x = self.upsample64(x)\n",
    "        x += x64\n",
    "        x = self.deconv32(x)\n",
    "        \n",
    "        x = self.upsample32(x)\n",
    "        x += x32\n",
    "        x = self.deconv1(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    @staticmethod\n",
    "    def _convBlock(in_channels, out_channels, kernel, stride, padding, activation):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, in_channels, kernel, stride, padding), activation,\n",
    "            nn.Conv2d(in_channels, in_channels, kernel, stride, padding), activation,\n",
    "            nn.Conv2d(in_channels, out_channels, kernel, stride, padding), activation\n",
    "        )\n",
    "    @staticmethod\n",
    "    def _upsampleBlock(upsample, in_channels, out_channels, kernel, stride, padding, activation):\n",
    "        return nn.Sequential(\n",
    "            upsample,\n",
    "            nn.Conv2d(in_channels, out_channels, kernel, stride, padding), activation\n",
    "        )\n",
    "\n",
    "model = Model()\n",
    "# print(model)\n",
    "# if device == torch.device(\"cpu\"):\n",
    "#     print(summary(model, (2,64,64)))\n",
    "# else:\n",
    "#     print(summary(model.cuda(), (2,64,64)))\n",
    "    \n",
    "# --------------------------------------------------------------------------------------------------\n",
    "\n",
    "def getLossOptimizer(net, learning_rate=0.001):\n",
    "    loss_function = nn.L1Loss()\n",
    "    optimizer = optim.Adamax(net.parameters(), lr=learning_rate)\n",
    "    \n",
    "    return (loss_function, optimizer)\n",
    "\n",
    "def getBatch(X, y, offset, batch_size, device):\n",
    "    input = torch.tensor( X[ offset:offset + batch_size ], dtype=torch.float)\n",
    "    target = torch.tensor( y[ offset:offset + batch_size ], dtype=torch.float)\n",
    "    return input.to(device), target.to(device)\n",
    "\n",
    "def formatTime(t):\n",
    "    s = t % 60\n",
    "    m = (t // 60) % 60\n",
    "    h = t // 3600\n",
    "    return str(h) + \":\" + str(m).zfill(2) + \":\" + str(s).zfill(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 462
    },
    "colab_type": "code",
    "id": "hOrq8Ntbj_EH",
    "outputId": "3de89e9d-de17-4e03-972f-f46c514b2ba6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== HYPERPARAMETERS ======\n",
      "batch_size = 128\n",
      "epochs = 2\n",
      "device cpu\n",
      "=============================\n",
      "===> Epoch[0]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/upsampling.py:129: UserWarning: nn.Upsample is deprecated. Use nn.functional.interpolate instead.\n",
      "  warnings.warn(\"nn.{} is deprecated. Use nn.functional.interpolate instead.\".format(self.name))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===> Epoch[0](3/494): Loss: 0.0902\tETA 1:05:36\tEpoch Loss: 0.0904"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-db41fc21568d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-db41fc21568d>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(model, device, X, y, batch_size, n_epochs, history, X_val, y_val)\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \"\"\"\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def fit(model, device, X, y, batch_size, n_epochs, history=None, X_val=None, y_val=None):\n",
    "    \n",
    "    #Print all of the hyperparameters of the training iteration:\n",
    "    print(\"====== HYPERPARAMETERS ======\")\n",
    "    print(\"batch_size =\", batch_size)\n",
    "    print(\"epochs =\", n_epochs)\n",
    "    print(\"device\",device)\n",
    "    print(\"=\" * 29)\n",
    "    \n",
    "    assert X.shape[0] == y.shape[0]\n",
    "    assert X.shape[2:4] == y.shape[2:4]\n",
    "    \n",
    "    model.to(device)\n",
    "    loss_function, optimizer = getLossOptimizer(model)\n",
    "    n_batch = X.shape[0] // batch_size\n",
    "    history = {\n",
    "        'train' : [],\n",
    "        'val' : []\n",
    "    }\n",
    "    \n",
    "    start_T = int(time.time())\n",
    "    tick_T = int(time.time())\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        print(\"===> Epoch[{}]\".format(epoch), end='', flush=True)\n",
    "        epoch_loss = 0\n",
    "        \n",
    "        for it in range(n_batch):\n",
    "            input, target = getBatch(X, y, it, batch_size, device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            output = model(input)\n",
    "            loss = loss_function(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            loss_val = loss.item()\n",
    "            epoch_loss += loss_val\n",
    "            \n",
    "            tack_T = int(time.time())\n",
    "            print(\"\\r\", end='')\n",
    "            print(\"===> Epoch[{}]({}/{}): Loss: {:.4f}\\tETA {}\\tEpoch Loss: {:.4f}\"\n",
    "                  .format(epoch, it, n_batch, loss_val,formatTime((tack_T - tick_T) * (n_batch - it + 1)),\n",
    "                  epoch_loss / (it+1)), end='', flush=True)\n",
    "            tick_T = tack_T\n",
    "            \n",
    "        epoch_loss /= n_batch\n",
    "        history['train'].append(epoch_loss)\n",
    "        print(\"Epoch[{}] finished with loss: {}\".format(epoch, epoch_loss))\n",
    "        \n",
    "        if X_val is not None and y_val is not None:\n",
    "            history['val'].append( validate(model, loss_function, X_val, y_val) )\n",
    "        \n",
    "        print(\"\\n----------------------------\")\n",
    "    print(\"Finished training of {} epochs in {}.\".format(n_epochs, formatTime(int(time.time())-start_T)))\n",
    "        \n",
    "    return history\n",
    "        \n",
    "def validate(model, loss_function, X_val, y_val):\n",
    "    X_val, y_val = val\n",
    "    assert X_val.shape[0] == y_val.shape[0]\n",
    "\n",
    "    total_loss_val = 0\n",
    "    n_batch_val = X_val.shape[0] // batch_size\n",
    "\n",
    "    print(\"Validating on {} samples.\".format(n_batch_val * batch_size))\n",
    "\n",
    "    for it in range(n_batch_val):\n",
    "        input, target = getBatch(X_val, y_val, it, batch_size, device)\n",
    "\n",
    "        output = model(input)\n",
    "        loss_val = loss_function(output, target)\n",
    "        total_loss_val += loss_val.data[0]\n",
    "\n",
    "    print(\"Validation loss = {:.4f}\".format(total_loss_val / n_batch_val))\n",
    "    return total_loss_val / n_batch_val\n",
    "\n",
    "\n",
    "history={}\n",
    "try:\n",
    "    fit(model, device, X_train, y_train, 128, 2, history, X_val=X_test, y_val=y_test)\n",
    "except:\n",
    "    print(\"\\n\\nFinished training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z8oKILPqAdMl"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lFUTyEdR1Mgs"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "12_pytorch.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
