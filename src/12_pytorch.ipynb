{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "12_pytorch.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "b3FAMmp61N9O",
        "outputId": "03350fa9-a086-4dd4-d4ba-4fb2d54a9671",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 714
        }
      },
      "cell_type": "code",
      "source": [
        "!pip3 install https://download.pytorch.org/whl/cu80/torch-1.0.0-cp36-cp36m-linux_x86_64.whl\n",
        "!pip3 install torchvision\n",
        "!pip3 install torchsummary\n",
        "!pip3 install tensorboardX\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "!cp gdrive/My\\ Drive/x64/*.npy .\n",
        "!cp gdrive/My\\ Drive/*.py .\n",
        "!ls -l"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torch==1.0.0 from https://download.pytorch.org/whl/cu80/torch-1.0.0-cp36-cp36m-linux_x86_64.whl\n",
            "\u001b[?25l  Downloading https://download.pytorch.org/whl/cu80/torch-1.0.0-cp36-cp36m-linux_x86_64.whl (532.5MB)\n",
            "\u001b[K    100% |████████████████████████████████| 532.5MB 31kB/s \n",
            "\u001b[31mfastai 1.0.50.post1 has requirement numpy>=1.15, but you'll have numpy 1.14.6 which is incompatible.\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch\n",
            "  Found existing installation: torch 1.0.1.post2\n",
            "    Uninstalling torch-1.0.1.post2:\n",
            "      Successfully uninstalled torch-1.0.1.post2\n",
            "Successfully installed torch-1.0.0\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (0.2.2.post3)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision) (4.1.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.11.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.14.6)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.0.0)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from pillow>=4.1.1->torchvision) (0.46)\n",
            "Requirement already satisfied: torchsummary in /usr/local/lib/python3.6/dist-packages (1.5.1)\n",
            "Collecting tensorboardX\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5c/76/89dd44458eb976347e5a6e75eb79fecf8facd46c1ce259bad54e0044ea35/tensorboardX-1.6-py2.py3-none-any.whl (129kB)\n",
            "\u001b[K    100% |████████████████████████████████| 133kB 4.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.14.6)\n",
            "Requirement already satisfied: protobuf>=3.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (3.7.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.11.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.2.0->tensorboardX) (40.8.0)\n",
            "Installing collected packages: tensorboardX\n",
            "Successfully installed tensorboardX-1.6\n",
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n",
            "total 949908\n",
            "-rw------- 1 root root      2875 Mar 30 20:36 _dataset_tools.py\n",
            "drwx------ 3 root root      4096 Mar 30 20:35 gdrive\n",
            "-rw------- 1 root root      2481 Mar 30 20:36 _my_tools.py\n",
            "drwxr-xr-x 1 root root      4096 Mar 27 20:26 sample_data\n",
            "-rw------- 1 root root 129687680 Mar 30 20:36 X_test.npy\n",
            "-rw------- 1 root root 518750336 Mar 30 20:36 X_train.npy\n",
            "-rw------- 1 root root  64843904 Mar 30 20:36 y_test.npy\n",
            "-rw------- 1 root root 259375232 Mar 30 20:36 y_train.npy\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "QG3xCwCI1Mfg",
        "outputId": "d6bee216-21d1-46eb-9352-27f33da9c176",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# from tensorboardX import SummaryWriter\n",
        "from torchsummary import summary\n",
        "\n",
        "import _my_tools as mt\n",
        "import numpy as np\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device is\", device)\n",
        "X_train, y_train, X_test, y_test = mt.loadData(\"\",'float16',channels_last=False)"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Device is cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "ScRT-TqfAAyg",
        "scrolled": true,
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Model, self).__init__()\n",
        "        \n",
        "        self.KERNEL_SIZE = 31\n",
        "        self.activation = nn.ReLU()\n",
        "        \n",
        "        self.conv_setup = {\n",
        "            'kernel' : (3,3),\n",
        "            'stride' : (1,1),\n",
        "            'padding' : 1,\n",
        "            'activation' : self.activation\n",
        "        }\n",
        "        self.pooling_setup = {\n",
        "            'kernel_size' : (2,2),\n",
        "            'stride' : (2,2)\n",
        "        }\n",
        "        self.upsample_setup = {\n",
        "            'scale_factor' : 2,\n",
        "            'mode' : 'bilinear',\n",
        "            'align_corners' : True\n",
        "        }\n",
        "\n",
        "        self.pooling_layer = nn.AvgPool2d(**self.pooling_setup)\n",
        "        self.upsample_layer = nn.Upsample(**self.upsample_setup)\n",
        "        \n",
        "        self.conv32 = self._convBlock(2, 32, **self.conv_setup)\n",
        "        self.conv64 = self._convBlock(32, 64, **self.conv_setup)\n",
        "        self.conv128 = self._convBlock(64, 128, **self.conv_setup)\n",
        "        self.conv256 = self._convBlock(128, 256, **self.conv_setup)\n",
        "        self.conv256_256 = self._convBlock(256, 256, **self.conv_setup)\n",
        "\n",
        "\n",
        "        self.upsample256 = self._upsampleBlock(self.upsample_layer, 256, 256, **self.conv_setup)\n",
        "        self.deconv128 = self._convBlock(256, 128, **self.conv_setup)\n",
        "        self.upsample128 = self._upsampleBlock(self.upsample_layer, 128, 128, **self.conv_setup)\n",
        "        self.deconv64 = self._convBlock(128, 64, **self.conv_setup)\n",
        "        self.upsample64 = self._upsampleBlock(self.upsample_layer, 64, 64, **self.conv_setup)\n",
        "        self.deconv32 = self._convBlock(64, 32, **self.conv_setup)\n",
        "        self.upsample32 = self._upsampleBlock(self.upsample_layer, 32, 32, **self.conv_setup)\n",
        "        self.deconv1 = self._convBlock(32, 1, **self.conv_setup)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x32 = self.conv32(x)\n",
        "        x32_p = self.pooling_layer(x32)\n",
        "#         x64 = self.conv64(x32_p)\n",
        "#         x64_p = self.pooling_layer(x64)\n",
        "#         x128 = self.conv128(x64_p)\n",
        "#         x128_p = self.pooling_layer(x128)\n",
        "#         x256 = self.conv256(x128_p)\n",
        "#         x256_p = self.pooling_layer(x256)\n",
        "\n",
        "#         x = self.conv256_256(x256_p)\n",
        "\n",
        "#         # expansion\n",
        "\n",
        "#         x = self.upsample256(x)\n",
        "#         x += x256\n",
        "#         x = self.deconv128(x)\n",
        "\n",
        "#         x = self.upsample128(x)\n",
        "#         x += x128\n",
        "#         x = self.deconv64(x)\n",
        "\n",
        "#         x = self.upsample64(x)\n",
        "#         x += x64\n",
        "#         x = self.deconv32(x)\n",
        "        \n",
        "        x = self.upsample32(x32_p)\n",
        "        x += x32\n",
        "        x = self.deconv1(x)\n",
        "        \n",
        "        return x\n",
        "    \n",
        "    @staticmethod\n",
        "    def _convBlock(in_channels, out_channels, kernel, stride, padding, activation):\n",
        "        return nn.Sequential(\n",
        "            nn.Conv2d(in_channels, in_channels, kernel, stride, padding), activation,\n",
        "            nn.Conv2d(in_channels, in_channels, kernel, stride, padding), activation,\n",
        "            nn.Conv2d(in_channels, out_channels, kernel, stride, padding), activation\n",
        "        )\n",
        "    @staticmethod\n",
        "    def _upsampleBlock(upsample, in_channels, out_channels, kernel, stride, padding, activation):\n",
        "        return nn.Sequential(\n",
        "            upsample,\n",
        "            nn.Conv2d(in_channels, out_channels, kernel, stride, padding), activation\n",
        "        )\n",
        "\n",
        "model = Model()\n",
        "model.to(device)\n",
        "# print(model)\n",
        "# if device == torch.device(\"cpu\"):\n",
        "#     print(summary(model, (2,64,64)))\n",
        "# else:\n",
        "#     print(summary(model.cuda(), (2,64,64)))\n",
        "    \n",
        "# --------------------------------------------------------------------------------------------------\n",
        "\n",
        "def getLossOptimizer(net, learning_rate=0.001):\n",
        "    loss_function = nn.L1Loss()\n",
        "    optimizer = optim.Adamax(net.parameters(), lr=learning_rate)\n",
        "    \n",
        "    return (loss_function, optimizer)\n",
        "\n",
        "def getBatch(X, y, offset, batch_size, device):\n",
        "    input = torch.tensor( X[ offset:offset + batch_size ], dtype=torch.float)\n",
        "    target = torch.tensor( y[ offset:offset + batch_size ], dtype=torch.float)\n",
        "    return input.to(device), target.to(device)\n",
        "\n",
        "def formatTime(t):\n",
        "    t = int(t)\n",
        "    s = t % 60\n",
        "    m = (t // 60) % 60\n",
        "    h = t // 3600\n",
        "    return str(h) + \":\" + str(m).zfill(2) + \":\" + str(s).zfill(2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "hOrq8Ntbj_EH",
        "outputId": "8189f024-8dc5-43c6-8c20-10c0a1b350de",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1148
        }
      },
      "cell_type": "code",
      "source": [
        "import time\n",
        "import copy\n",
        "\n",
        "def fit(model, device, X, y, batch_size, n_epochs, history, X_val=None, y_val=None):\n",
        "    \n",
        "    #Print all of the hyperparameters of the training iteration:\n",
        "    print(\"====== HYPERPARAMETERS ======\")\n",
        "    print(\"batch_size =\", batch_size)\n",
        "    print(\"epochs =\", n_epochs)\n",
        "    print(\"device\",device)\n",
        "    print(\"=\" * 29)\n",
        "    \n",
        "    assert X.shape[0] == y.shape[0]\n",
        "    assert X.shape[2:4] == y.shape[2:4]\n",
        "    \n",
        "    model.to(device)\n",
        "    loss_function, optimizer = getLossOptimizer(model)\n",
        "    n_batch = X.shape[0] // batch_size\n",
        "    \n",
        "    start_T = int(time.time())\n",
        "    tick_T = time.time()\n",
        "    \n",
        "    for epoch in range(n_epochs):\n",
        "        print(\"===> Epoch[{}]\".format(epoch), end='', flush=True)\n",
        "        epoch_loss = 0\n",
        "        \n",
        "        for it in range(n_batch):\n",
        "            input, target = getBatch(X, y, it, batch_size, device)\n",
        "            print(\"##\",torch.mean(input))\n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            output = model(input)\n",
        "            print(torch.max(output))\n",
        "            loss = loss_function(output, target)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            loss_train = loss.item()\n",
        "            epoch_loss += loss_train\n",
        "            \n",
        "            tack_T = time.time()\n",
        "            print(\"\\r\", end='')\n",
        "            print(\"===> Epoch[{}]({}/{}): Loss: {:.4f}\\tETA {}\\tEpoch Loss: {:.4f}\"\n",
        "                  .format(epoch, it + 1, n_batch, loss_train,\n",
        "                  formatTime((tack_T - start_T) / (it + 1) * (n_batch - it + 1)),\n",
        "                  epoch_loss / (it+1)), end='', flush=True)\n",
        "            tick_T = tack_T\n",
        "            \n",
        "        epoch_loss /= n_batch\n",
        "        history['train'].append(epoch_loss)\n",
        "        print(\"\\nEpoch[{}] finished with loss: {}\".format(epoch, epoch_loss))\n",
        "        \n",
        "        if X_val is not None and y_val is not None:\n",
        "            history['val'].append( validate(model, loss_function, X_val, y_val, batch_size) )\n",
        "        \n",
        "        print(\"\\n----------------------------\\n\")\n",
        "    print(\"Finished training of {} epochs in {}.\".format(n_epochs, formatTime(int(time.time())-start_T)))\n",
        "        \n",
        "    return history\n",
        "        \n",
        "def validate(model, loss_function, X_val, y_val, train_batch_size):\n",
        "    assert X_val.shape[0] == y_val.shape[0]\n",
        "\n",
        "    val_loss = 0\n",
        "    n_batch_val = X_val.shape[0] // train_batch_size\n",
        "\n",
        "    print(\"Validating on {} samples.\".format(n_batch_val * train_batch_size))\n",
        "\n",
        "    for it in range(n_batch_val):\n",
        "        input, target = getBatch(X_val, y_val, it, train_batch_size, device)\n",
        "\n",
        "        output = model(input)\n",
        "        loss = loss_function(output, target)\n",
        "        val_loss += loss.item()\n",
        "\n",
        "    print(\"Validation loss = {:.4f}\".format(val_loss / n_batch_val))\n",
        "    return val_loss / n_batch_val\n",
        "\n",
        "\n",
        "history = {\n",
        "    'train' : [],\n",
        "    'val' : []\n",
        "}\n",
        "try:\n",
        "    fit(model, device, X_train, y_train, 128, 5, history, X_val=X_test, y_val=y_test)\n",
        "except KeyboardInterrupt:\n",
        "    print(\"\\n\\nFinished training.\")"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "====== HYPERPARAMETERS ======\n",
            "batch_size = 128\n",
            "epochs = 5\n",
            "device cuda:0\n",
            "=============================\n",
            "===> Epoch[0]## tensor(0.0889, device='cuda:0')\n",
            "tensor(0.0271, device='cuda:0', grad_fn=<MaxBackward1>)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/upsampling.py:129: UserWarning: nn.Upsample is deprecated. Use nn.functional.interpolate instead.\n",
            "  warnings.warn(\"nn.{} is deprecated. Use nn.functional.interpolate instead.\".format(self.name))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\r===> Epoch[0](1/494): Loss: 0.0915\tETA 0:03:15\tEpoch Loss: 0.0915## tensor(0.0892, device='cuda:0')\n",
            "tensor(0.0153, device='cuda:0', grad_fn=<MaxBackward1>)\n",
            "===> Epoch[0](2/494): Loss: 0.0894\tETA 0:02:28\tEpoch Loss: 0.0904## tensor(0.0893, device='cuda:0')\n",
            "tensor(0.0119, device='cuda:0', grad_fn=<MaxBackward1>)\n",
            "===> Epoch[0](3/494): Loss: 0.0895\tETA 0:02:09\tEpoch Loss: 0.0901## tensor(0.0902, device='cuda:0')\n",
            "tensor(0.0088, device='cuda:0', grad_fn=<MaxBackward1>)\n",
            "===> Epoch[0](4/494): Loss: 0.0904\tETA 0:01:58\tEpoch Loss: 0.0902## tensor(0.0899, device='cuda:0')\n",
            "tensor(0.0064, device='cuda:0', grad_fn=<MaxBackward1>)\n",
            "===> Epoch[0](5/494): Loss: 0.0901\tETA 0:01:51\tEpoch Loss: 0.0902## tensor(0.0895, device='cuda:0')\n",
            "tensor(0.0046, device='cuda:0', grad_fn=<MaxBackward1>)\n",
            "===> Epoch[0](6/494): Loss: 0.0896\tETA 0:01:46\tEpoch Loss: 0.0901## tensor(0.0905, device='cuda:0')\n",
            "tensor(0.0032, device='cuda:0', grad_fn=<MaxBackward1>)\n",
            "===> Epoch[0](7/494): Loss: 0.0907\tETA 0:01:42\tEpoch Loss: 0.0902## tensor(0.0899, device='cuda:0')\n",
            "tensor(0.0020, device='cuda:0', grad_fn=<MaxBackward1>)\n",
            "===> Epoch[0](8/494): Loss: 0.0900\tETA 0:01:39\tEpoch Loss: 0.0901## tensor(0.0900, device='cuda:0')\n",
            "tensor(0.0010, device='cuda:0', grad_fn=<MaxBackward1>)\n",
            "===> Epoch[0](9/494): Loss: 0.0902\tETA 0:01:37\tEpoch Loss: 0.0902## tensor(0.0900, device='cuda:0')\n",
            "tensor(0.0002, device='cuda:0', grad_fn=<MaxBackward1>)\n",
            "===> Epoch[0](10/494): Loss: 0.0902\tETA 0:01:35\tEpoch Loss: 0.0902## tensor(0.0908, device='cuda:0')\n",
            "tensor(0., device='cuda:0', grad_fn=<MaxBackward1>)\n",
            "===> Epoch[0](11/494): Loss: 0.0910\tETA 0:01:34\tEpoch Loss: 0.0902## tensor(0.0906, device='cuda:0')\n",
            "tensor(0., device='cuda:0', grad_fn=<MaxBackward1>)\n",
            "===> Epoch[0](12/494): Loss: 0.0908\tETA 0:01:32\tEpoch Loss: 0.0903## tensor(0.0905, device='cuda:0')\n",
            "tensor(0., device='cuda:0', grad_fn=<MaxBackward1>)\n",
            "===> Epoch[0](13/494): Loss: 0.0907\tETA 0:01:31\tEpoch Loss: 0.0903## tensor(0.0906, device='cuda:0')\n",
            "tensor(0., device='cuda:0', grad_fn=<MaxBackward1>)\n",
            "===> Epoch[0](14/494): Loss: 0.0907\tETA 0:01:30\tEpoch Loss: 0.0903## tensor(0.0899, device='cuda:0')\n",
            "tensor(0., device='cuda:0', grad_fn=<MaxBackward1>)\n",
            "===> Epoch[0](15/494): Loss: 0.0899\tETA 0:01:30\tEpoch Loss: 0.0903## tensor(0.0897, device='cuda:0')\n",
            "tensor(0., device='cuda:0', grad_fn=<MaxBackward1>)\n",
            "===> Epoch[0](16/494): Loss: 0.0898\tETA 0:01:29\tEpoch Loss: 0.0903## tensor(0.0894, device='cuda:0')\n",
            "tensor(0., device='cuda:0', grad_fn=<MaxBackward1>)\n",
            "===> Epoch[0](17/494): Loss: 0.0895\tETA 0:01:28\tEpoch Loss: 0.0902## tensor(0.0888, device='cuda:0')\n",
            "tensor(0., device='cuda:0', grad_fn=<MaxBackward1>)\n",
            "===> Epoch[0](18/494): Loss: 0.0888\tETA 0:01:28\tEpoch Loss: 0.0901## tensor(0.0874, device='cuda:0')\n",
            "tensor(0., device='cuda:0', grad_fn=<MaxBackward1>)\n",
            "===> Epoch[0](19/494): Loss: 0.0874\tETA 0:01:27\tEpoch Loss: 0.0900## tensor(0.0872, device='cuda:0')\n",
            "tensor(0., device='cuda:0', grad_fn=<MaxBackward1>)\n",
            "===> Epoch[0](20/494): Loss: 0.0872\tETA 0:01:27\tEpoch Loss: 0.0899## tensor(0.0881, device='cuda:0')\n",
            "tensor(0., device='cuda:0', grad_fn=<MaxBackward1>)\n",
            "===> Epoch[0](21/494): Loss: 0.0880\tETA 0:01:26\tEpoch Loss: 0.0898## tensor(0.0886, device='cuda:0')\n",
            "tensor(0., device='cuda:0', grad_fn=<MaxBackward1>)\n",
            "===> Epoch[0](22/494): Loss: 0.0886\tETA 0:01:26\tEpoch Loss: 0.0897## tensor(0.0884, device='cuda:0')\n",
            "tensor(0., device='cuda:0', grad_fn=<MaxBackward1>)\n",
            "===> Epoch[0](23/494): Loss: 0.0885\tETA 0:01:25\tEpoch Loss: 0.0897## tensor(0.0888, device='cuda:0')\n",
            "tensor(0., device='cuda:0', grad_fn=<MaxBackward1>)\n",
            "===> Epoch[0](24/494): Loss: 0.0888\tETA 0:01:25\tEpoch Loss: 0.0896## tensor(0.0885, device='cuda:0')\n",
            "tensor(0., device='cuda:0', grad_fn=<MaxBackward1>)\n",
            "===> Epoch[0](25/494): Loss: 0.0886\tETA 0:01:24\tEpoch Loss: 0.0896## tensor(0.0881, device='cuda:0')\n",
            "tensor(0., device='cuda:0', grad_fn=<MaxBackward1>)\n",
            "===> Epoch[0](26/494): Loss: 0.0882\tETA 0:01:24\tEpoch Loss: 0.0895## tensor(0.0896, device='cuda:0')\n",
            "tensor(0., device='cuda:0', grad_fn=<MaxBackward1>)\n",
            "===> Epoch[0](27/494): Loss: 0.0897\tETA 0:01:24\tEpoch Loss: 0.0895\n",
            "\n",
            "Finished training.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "Z8oKILPqAdMl",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(history)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "lFUTyEdR1Mgs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 522
        },
        "outputId": "912fafeb-b470-4212-ea23-10ecfdb6bfe0"
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "def showImgGC(name,*img,folder=None):\n",
        "\tif len(img) == 0:\n",
        "\t\treturn\n",
        "\telse:\n",
        "\t\tres = img[0]\n",
        "\t\tfor i in img[1:]:\n",
        "\t\t\tres = np.concatenate((res,i),axis=1)\n",
        "\t\t\t\n",
        "\tfig, ax = plt.subplots(figsize=(30,30))\n",
        "\tax.grid(False)\n",
        "\tax.imshow(res.squeeze(), cmap='binary_r')\n",
        "\tif folder is not None:\n",
        "\t\tres = (res * 255).astype('int')\n",
        "\t\tcv.imwrite(folder+name+\".png\",res)\n",
        "\n",
        "def compare(i,X,y,res,folder=None):\n",
        "\tshowImgGC(str(i).zfill(2),X[i,0,:,:],y[i,0,:,:],res[i,0,:,:],X[i,1,:,:],folder=folder)\n",
        "\n",
        "a = model(torch.tensor(X_test[0:10], dtype=torch.float).to(device))\n",
        "compare(0, X_test, y_test, a.cpu().detach().numpy())"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/upsampling.py:129: UserWarning: nn.Upsample is deprecated. Use nn.functional.interpolate instead.\n",
            "  warnings.warn(\"nn.{} is deprecated. Use nn.functional.interpolate instead.\".format(self.name))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABqoAAAHCCAYAAACXE5hCAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 2160x2160 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "n3Abadhs8nwP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}