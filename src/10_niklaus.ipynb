{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "10_niklaus.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "mppKeZOQlJau",
        "outputId": "de57c46f-0f21-4f6a-c86f-995852385384",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 635
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install tensorflow-gpu\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "!mkdir -p x64\n",
        "!cp gdrive/My\\ Drive/x64/*.npy x64/.\n",
        "!cp gdrive/My\\ Drive/*.py .\n",
        "!ls -l"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow-gpu in /usr/local/lib/python3.6/dist-packages (1.13.1)\n",
            "Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.7.1)\n",
            "Requirement already satisfied: tensorflow-estimator<1.14.0rc0,>=1.13.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.13.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.33.1)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (3.7.0)\n",
            "Requirement already satisfied: tensorboard<1.14.0,>=1.13.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.13.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.1.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.15.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.0.7)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.0.9)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.11.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.7.1)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.2.2)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.14.6)\n",
            "Requirement already satisfied: mock>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-estimator<1.14.0rc0,>=1.13.0->tensorflow-gpu) (2.0.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow-gpu) (40.8.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow-gpu) (0.14.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow-gpu) (3.0.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow-gpu) (2.8.0)\n",
            "Requirement already satisfied: pbr>=0.11 in /usr/local/lib/python3.6/dist-packages (from mock>=2.0.0->tensorflow-estimator<1.14.0rc0,>=1.13.0->tensorflow-gpu) (5.1.3)\n",
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "total 5395780\n",
            "-rw------- 1 root root       2875 Mar 27 13:32 _dataset_tools.py\n",
            "drwx------ 3 root root       4096 Mar 27 11:45 gdrive\n",
            "-rw-r--r-- 1 root root          0 Mar 27 13:13 history\n",
            "-rw-r--r-- 1 root root     319586 Mar 27 13:19 model.png\n",
            "-rw------- 1 root root       2398 Mar 27 13:32 _my_tools.py\n",
            "drwxr-xr-x 2 root root       4096 Mar 27 11:47 __pycache__\n",
            "drwxr-xr-x 1 root root       4096 Mar  8 17:26 sample_data\n",
            "drwxr-xr-x 2 root root       4096 Mar 27 13:32 x64\n",
            "-rw------- 1 root root  920825984 Mar 27 11:45 X_test.npy\n",
            "-rw------- 1 root root 2762440832 Mar 27 11:46 X_train.npy\n",
            "-rw------- 1 root root  460413056 Mar 27 11:46 y_test.npy\n",
            "-rw------- 1 root root 1381220480 Mar 27 11:46 y_train.npy\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "JuEl6_bckw2t",
        "outputId": "0c5502f7-6244-4236-916e-a664271cb7a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import _my_tools as mt\n",
        "import numpy as np\n",
        "\n",
        "X_train, y_train, X_test, y_test = mt.loadData(\"x64/\",'float16',channels_last=True)\n",
        "\n",
        "print(\"tf version - \", tf.__version__)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf version -  1.13.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "2IV3d8PqpBXk",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Layer, InputSpec\n",
        "\n",
        "# It assumes, that we are using channel_last.\n",
        "class SepConv(Layer):\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        super(SepConv, self).__init__(**kwargs)\n",
        "        self.input_spec = [InputSpec(ndim=4), InputSpec(ndim=4),InputSpec(ndim=4)]\n",
        "\n",
        "        self.sess = tf.Session()\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        assert isinstance(input_shape, list)\n",
        "        assert (len(input_shape) == 3)\n",
        "\n",
        "        self.input_spec = [InputSpec(shape=input_shape[0]), InputSpec(shape=input_shape[1]), InputSpec(shape=input_shape[2])]\n",
        "        self.checkInput()\n",
        "        \n",
        "        super(SepConv, self).build(input_shape)  # Be sure to call this at the end\n",
        "\n",
        "    def call(self, x):\n",
        "        assert isinstance(x, list)\n",
        "        assert (len(x) == 3)\n",
        "        \n",
        "        # Unpack the batch dimension\n",
        "#        print(\"--> SepConv.call()\")\n",
        "        output = tf.map_fn(self.l_1, x, dtype=tf.float32)\n",
        "        \n",
        "        return output\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        assert isinstance(input_shape, list)\n",
        "        assert (len(input_shape) == 3)\n",
        "\n",
        "        img, horizontal, vertical = input_shape\n",
        "        \n",
        "        return [self.n_b, self.m_out, self.m_out, self.n_channels]\n",
        "\n",
        "    def checkInput(self):#, img, horizontal, vertical):\n",
        "        i_shape = self.input_spec[0].shape\n",
        "        h_shape = self.input_spec[1].shape\n",
        "        v_shape = self.input_spec[2].shape\n",
        "    \n",
        "        self.n_b = i_shape[0] # number of batches\n",
        "        self.m = i_shape[1] # dimension of image\n",
        "        self.n_channels = i_shape[3] # number of channels\n",
        "        self.filter_size = h_shape[3] # size of filter == depth of NN output\n",
        "        self.m_out = self.m - self.filter_size + 1\n",
        "        \n",
        "        assert i_shape[1] == i_shape[2]\n",
        "        assert h_shape[3] == v_shape[3]\n",
        "        assert h_shape[1] == h_shape[2] == v_shape[1] == v_shape[2] == self.m_out\n",
        "    \n",
        "#     def localSepConv(self, x):\n",
        "#         img, horizontal, vertical = x\n",
        "#         output = np.zeros([self.m_out, self.m_out, self.n_channels])\n",
        "        \n",
        "#         for row in range(self.m_out):\n",
        "#             for col in range(self.m_out):\n",
        "#                 sub_patch = img[row:row + self.filter_size, col:col + self.filter_size, :]\n",
        "# #                 print(sess.run(tf.shape(sub_patch)))\n",
        "#                 kernel_h = tf.reshape( horizontal[row,col,:] , [1,self.filter_size,self.n_channels] )\n",
        "# #                 print(sess.run(kernel_h))\n",
        "# #                 kernel_v = tf.transpose(vertical[row,col,:])\n",
        "#                 kernel_v = tf.reshape( vertical[row,col,:] , [self.filter_size,1,self.n_channels] )\n",
        "# #                 print(sess.run(kernel_v))\n",
        "# #                 print(sess.run( tf.shape(tf.reduce_sum(sub_patch * kernel_h * kernel_v,[0,1]) )))\n",
        "#                 output[row,col,:] = tf.reduce_sum(sub_patch * kernel_h * kernel_v,[0,1])#.eval(session=self.sess)\n",
        "                \n",
        "#         return tf.convert_to_tensor(output)\n",
        "    \n",
        "    # l_1() unpacks the 1st dimension\n",
        "    # r is list [0,1,...,filter size], we will cut out the subpatch of input image according to this list\n",
        "    def l_1(self, x):\n",
        "        img, horizontal, vertical = x\n",
        "        \n",
        "        r = tf.convert_to_tensor(list(range(self.m_out)))\n",
        "        \n",
        "        output = tf.map_fn(lambda x: self.l_2(img,x), [horizontal,vertical,r], dtype=tf.float32)\n",
        "        \n",
        "        return output\n",
        "        \n",
        "    # l_2() unpacks the 2nd dimension\n",
        "    # sub_patch_tmp is sub patch of input image according to 1st dimension\n",
        "    def l_2(self, img, x):\n",
        "        horizontal, vertical, i = x\n",
        "\n",
        "        sub_patch_tmp = img[i:i + self.filter_size,:,:]\n",
        "        r = tf.convert_to_tensor(list(range(self.m_out)))\n",
        "        \n",
        "        output = tf.map_fn(lambda x: self.l_3(sub_patch_tmp,x),[horizontal, vertical,r], dtype=tf.float32)\n",
        "        \n",
        "        return output\n",
        "    \n",
        "    # sub_patch is sub patch of input image according to 1st and 2nd dimension\n",
        "    # l_3() performs convolution on sub_patch and kernels obtained from unpacking of input tensors\n",
        "    def l_3(self, img, x):\n",
        "        horizontal, vertical, i = x\n",
        "        \n",
        "        sub_patch = img[:,i:i + self.filter_size,:]\n",
        "        kernel_h = tf.reshape(horizontal,[1, self.filter_size, 1])\n",
        "        kernel_v = tf.reshape(vertical, [self.filter_size, 1, 1])\n",
        "        \n",
        "        output = tf.reduce_sum(sub_patch * kernel_h * kernel_v, [0,1])\n",
        "        \n",
        "        return output\n",
        "    \n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "class padRep(Layer):\n",
        "    \n",
        "    def __init__(self, im, depth, **kwargs):\n",
        "        super(padRep, self).__init__(**kwargs)\n",
        "        \n",
        "        self.im = im # image / channel from input, that we want to extract and pad\n",
        "        self.depth = depth # depth of padding on every side of input\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        super(padRep, self).build(input_shape)  # Be sure to call this at the end\n",
        "\n",
        "    def call(self, x):\n",
        "        i = 1\n",
        "        d = self.depth\n",
        "        res = x[:,:,:,self.im:self.im + 1]\n",
        "        \n",
        "        while d > 0 :\n",
        "            if i <= d :\n",
        "                res = tf.pad(res, [[0,0],[i,i],[i,i],[0,0]], \"SYMMETRIC\")\n",
        "                d -= i\n",
        "            else:\n",
        "                res = tf.pad(res, [[0,0],[d,d],[d,d],[0,0]], \"SYMMETRIC\")\n",
        "                d -= d\n",
        "            i *= 2\n",
        "        \n",
        "        return res\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return (input_shape[0], input_shape[1] + 2 * self.depth, input_shape[2] + 2 * self.depth, 1)\n",
        "      \n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def convBlock(in_channels, out_channels, kernel, stride, padding, activation):\n",
        "    return keras.models.Sequential([\n",
        "        keras.layers.Conv2D(in_channels, kernel, stride, padding, activation=activation),\n",
        "        keras.layers.Conv2D(in_channels, kernel, stride, padding, activation=activation),\n",
        "        keras.layers.Conv2D(out_channels, kernel, stride, padding, activation=activation)\n",
        "    ], name='convBlock' + str(in_channels) + '_' + str(out_channels))\n",
        "\n",
        "def upsampleBlock(upsample, in_channels, out_channels, kernel, stride, padding, activation):\n",
        "    return keras.models.Sequential([\n",
        "        upsample,\n",
        "        keras.layers.Conv2D(out_channels, kernel, stride, padding, activation=activation)\n",
        "    ], name='upsampleBlock' + str(in_channels) + '_' + str(out_channels))\n",
        "\n",
        "def kernelConvBlock(upsample, in_channels, out_channels, kernel, stride, padding, activation, name=None):\n",
        "    return keras.models.Sequential([\n",
        "        keras.layers.Conv2D(in_channels, kernel, stride, padding, activation=activation),\n",
        "        keras.layers.Conv2D(in_channels, kernel, stride, padding, activation=activation),\n",
        "        keras.layers.Conv2D(out_channels, kernel, stride, padding, activation=activation),\n",
        "        upsample,\n",
        "        keras.layers.Conv2D(out_channels, kernel, stride, padding, activation=activation)\n",
        "    ], name=name)\n",
        "\n",
        "def pool(pool_size, padding):\n",
        "    return keras.layers.AveragePooling2D(pool_size, padding=padding)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "scrolled": false,
        "id": "MF_ldWBG9npx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "KERNEL_SIZE = 31\n",
        "conv_setup = {\n",
        "    'kernel' : (3,3),\n",
        "    'stride' : (1,1),\n",
        "    'padding' : 'same',\n",
        "    'activation' : 'relu'\n",
        "}\n",
        "pooling_setup = {\n",
        "    'pool_size' : (2,2),\n",
        "    'padding' : 'same'\n",
        "}\n",
        "upsample_setup = {\n",
        "    'size' : (2,2),\n",
        "    'interpolation' : 'bilinear'\n",
        "}\n",
        "\n",
        "pooling_layer = keras.layers.AveragePooling2D(**pooling_setup)\n",
        "upsample_layer = keras.layers.UpSampling2D(**upsample_setup)\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "input_img = keras.layers.Input((64,64,2))\n",
        "\n",
        "# compression\n",
        "\n",
        "x32 = convBlock(2, 32, **conv_setup)(input_img)\n",
        "x32_p = pool(**pooling_setup)(x32)\n",
        "x64 = convBlock(32, 64, **conv_setup)(x32_p)\n",
        "x64_p = pool(**pooling_setup)(x64)\n",
        "x128 = convBlock(64, 128, **conv_setup)(x64_p)\n",
        "x128_p = pool(**pooling_setup)(x128)\n",
        "x256 = convBlock(128, 256, **conv_setup)(x128_p)\n",
        "x256_p = pool(**pooling_setup)(x256)\n",
        "# x512 = convBlock(256, 512, **conv_setup)(x256_p)\n",
        "# x512_p = pool(**pooling_setup)(x512)\n",
        "\n",
        "# x = convBlock(512, 512, **conv_setup)(x512_p)\n",
        "\n",
        "x = convBlock(256, 256, **conv_setup)(x256_p)\n",
        "\n",
        "# expansion\n",
        "\n",
        "# x = upsampleBlock(upsample_layer, 512, 512, **conv_setup)(x)\n",
        "# x = keras.layers.Add()([x,x512])\n",
        "# x = convBlock(512, 256, **conv_setup)(x)\n",
        "\n",
        "x = upsampleBlock(upsample_layer, 256, 256, **conv_setup)(x)\n",
        "x = keras.layers.Add()([x,x256])\n",
        "x = convBlock(256, 128, **conv_setup)(x)\n",
        "\n",
        "x = upsampleBlock(upsample_layer, 128, 128, **conv_setup)(x)\n",
        "x = keras.layers.Add()([x,x128])\n",
        "x = convBlock(128, 64, **conv_setup)(x)\n",
        "\n",
        "x = upsampleBlock(upsample_layer, 64, 64, **conv_setup)(x)\n",
        "x = keras.layers.Add()([x,x64])\n",
        "\n",
        "# estimation of local convolution kernels\n",
        "\n",
        "k1h = kernelConvBlock(upsample_layer, 64, KERNEL_SIZE, **conv_setup, name='kernelConvBlock1')(x)\n",
        "k1v = kernelConvBlock(upsample_layer, 64, KERNEL_SIZE, **conv_setup, name='kernelConvBlock2')(x)\n",
        "k2h = kernelConvBlock(upsample_layer, 64, KERNEL_SIZE, **conv_setup, name='kernelConvBlock3')(x)\n",
        "k2v = kernelConvBlock(upsample_layer, 64, KERNEL_SIZE, **conv_setup, name='kernelConvBlock4')(x)\n",
        "\n",
        "im1 = padRep(0, KERNEL_SIZE//2)(input_img)\n",
        "im2 = padRep(1, KERNEL_SIZE//2)(input_img)\n",
        "\n",
        "out1 = SepConv()([im1,k1h,k1v])\n",
        "out2 = SepConv()([im2,k2h,k2v])\n",
        "\n",
        "output = keras.layers.Add()([out1,out2])\n",
        "\n",
        "model = keras.Model(input_img, output)\n",
        "model.compile(optimizer='adadelta', loss='mean_absolute_error')\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "from keras.utils import plot_model\n",
        "plot_model(model, to_file='model.png', show_shapes=True, show_layer_names=True)\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2DV_Q6BoYbwJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "a = model.predict(X_train[0:1])\n",
        "mt.compare(0, X_train, y_train, a)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SVZCC0-gPbJv",
        "colab_type": "code",
        "outputId": "be2e208c-78fd-4514-c696-cebb81a5a07a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "cell_type": "code",
      "source": [
        "checkpoint_path = \"10/cp\"\n",
        "cp_callback = keras.callbacks.ModelCheckpoint(checkpoint_path,\n",
        "                                             save_weights_only=True,\n",
        "                                             verbose=1)\n",
        "csv_callback = keras.callbacks.CSVLogger('history', separator=',', append=True)\n",
        "\n",
        "batch_size = 16\n",
        "num_epoch = 1\n",
        "#model training\n",
        "model_log = model.fit(X_train, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=num_epoch,\n",
        "          verbose=1,          \n",
        "          validation_data=(X_test, y_test),\n",
        "          callbacks = [cp_callback])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 63324 samples, validate on 15831 samples\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "tCuz1XpZokwz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Small model test"
      ]
    },
    {
      "metadata": {
        "id": "7qDgQupboovI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 437
        },
        "outputId": "650d21ca-869d-480e-b867-fe66e1fda68b"
      },
      "cell_type": "code",
      "source": [
        "KERNEL_SIZE = 31\n",
        "conv_setup = {\n",
        "    'kernel' : (3,3),\n",
        "    'stride' : (1,1),\n",
        "    'padding' : 'same',\n",
        "    'activation' : 'relu'\n",
        "}\n",
        "pooling_setup = {\n",
        "    'pool_size' : (2,2),\n",
        "    'padding' : 'same'\n",
        "}\n",
        "upsample_setup = {\n",
        "    'size' : (2,2),\n",
        "    'interpolation' : 'bilinear'\n",
        "}\n",
        "\n",
        "pooling_layer = keras.layers.AveragePooling2D(**pooling_setup)\n",
        "upsample_layer = keras.layers.UpSampling2D(**upsample_setup)\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "input_img = keras.layers.Input((64,64,2))\n",
        "\n",
        "# compression\n",
        "\n",
        "# x32 = convBlock(2, 32, **conv_setup)(input_img)\n",
        "x = pool(**pooling_setup)(input_img)\n",
        "\n",
        "k1h = kernelConvBlock(upsample_layer, 64, KERNEL_SIZE, **conv_setup, name='kernelConvBlock1')(x)\n",
        "# k1v = kernelConvBlock(upsample_layer, 64, KERNEL_SIZE, **conv_setup, name='kernelConvBlock2')(x)\n",
        "# k2h = kernelConvBlock(upsample_layer, 64, KERNEL_SIZE, **conv_setup, name='kernelConvBlock3')(x)\n",
        "# k2v = kernelConvBlock(upsample_layer, 64, KERNEL_SIZE, **conv_setup, name='kernelConvBlock4')(x)\n",
        "\n",
        "im1 = padRep(0, KERNEL_SIZE//2)(input_img)\n",
        "# im2 = padRep(1, KERNEL_SIZE//2)(input_img)\n",
        "\n",
        "out1 = SepConv()([im1,k1h,k1h])\n",
        "# out2 = SepConv()([im2,k2h,k2v])\n",
        "\n",
        "# output = keras.layers.Add()([out1,out2])\n",
        "\n",
        "model_small = keras.Model(input_img, out1)\n",
        "model_small.compile(optimizer='adadelta', loss='mean_absolute_error')\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "# from keras.utils import plot_model\n",
        "# plot_model(model, to_file='model.png', show_shapes=True, show_layer_names=True)\n",
        "model_small.summary()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 64, 64, 2)    0                                            \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_1 (AveragePoo (None, 32, 32, 2)    0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "pad_rep (padRep)                (None, 94, 94, 1)    0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "kernelConvBlock1 (Sequential)   (None, 64, 64, 31)   64711       average_pooling2d_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "sep_conv (SepConv)              (None, 64, 64, 1)    0           pad_rep[0][0]                    \n",
            "                                                                 kernelConvBlock1[0][0]           \n",
            "                                                                 kernelConvBlock1[0][0]           \n",
            "==================================================================================================\n",
            "Total params: 64,711\n",
            "Trainable params: 64,711\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "o2Q1YO2Cpcjb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 827
        },
        "outputId": "dc6bbc5d-dd91-4b63-eca7-b2be2bb8dde9"
      },
      "cell_type": "code",
      "source": [
        "checkpoint_path = \"10_small/cp\"\n",
        "cp_callback = keras.callbacks.ModelCheckpoint(checkpoint_path,\n",
        "                                             save_weights_only=True,\n",
        "                                             verbose=1)\n",
        "csv_callback = keras.callbacks.CSVLogger('history', separator=',', append=True)\n",
        "\n",
        "batch_size = 16\n",
        "num_epoch = 1\n",
        "#model training\n",
        "model_log = model_small.fit(X_train, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=num_epoch,\n",
        "          verbose=1,          \n",
        "          validation_data=(X_test, y_test),\n",
        "          callbacks = [cp_callback])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 63324 samples, validate on 15831 samples\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "   48/63324 [..............................] - ETA: 121:32:20 - loss: 1.1633"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-f8c68d0cad6f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m           \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m           \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m           callbacks = [cp_callback])\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    878\u001b[0m           \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m           \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m           validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m   def evaluate(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, mode, validation_in_fit, **kwargs)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m         \u001b[0;31m# Get outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 329\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    330\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3074\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3075\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3076\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3077\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3078\u001b[0m     return nest.pack_sequence_as(self._outputs_structure,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "Vqn1QeEcrN-R",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model_small.get_weights()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ye6QRdfPp2Wy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "outputId": "4d2b8b97-ebac-45ff-c6b1-5134e2cef951"
      },
      "cell_type": "code",
      "source": [
        "a = model_small.predict(X_train[0:1])\n",
        "mt.compare(0, X_train, y_train, a)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABqoAAAHCCAYAAACXE5hCAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 2160x2160 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "qQ5AVoWh9np2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Le test"
      ]
    },
    {
      "metadata": {
        "id": "wOLVJn0b9np3",
        "colab_type": "code",
        "outputId": "9fa97f61-0a59-4d5f-e4ff-52c8835f52e4",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "L = SepConv()\n",
        "sess = tf.Session()\n",
        "L.build([[None,6,6,1],[None,4,4,3],[None,4,4,3]])\n",
        "\n",
        "A = tf.reshape(tf.constant([[0,0,1,2,3,3],[0,0,1,2,3,3],[1,1,2,3,4,4],[2,2,3.,4,5,5],[3,3,4,5,6,6],[3,3,4,5,6,6]]),[1,6,6,1])\n",
        "B = tf.reshape(tf.constant([[[0.,1,2],[0.,1,2],[0.,1,2],[0.,1,2]],[[0.,1,2],[0.,1,2],[0.,1,2],[0.,1,2]],[[0.,1,2],[0.,1,2],[0.,1,2],[0.,1,2]],[[0.,1,2],[0.,1,2],[0.,1,2],[0.,1,2]]]),[1,4,4,3])\n",
        "C = tf.reshape(tf.constant([[[-1.,0,1],[-1.,0,1],[-1.,0,1],[-1.,0,1]],[[-1.,0,1],[-1.,0,1],[-1.,0,1],[-1.,0,1]],[[-1.,0,1],[-1.,0,1],[-1.,0,1],[-1.,0,1]],[[-1.,0,1],[-1.,0,1],[-1.,0,1],[-1.,0,1]]]),[1,4,4,3])\n",
        "\n",
        "\n",
        "D = L.call([A,B,C])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[None, 6, 6, 1], [None, 4, 4, 3], [None, 4, 4, 3]]\n",
            "here -  6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "dhX36V8f9np7",
        "colab_type": "code",
        "outputId": "4889315c-e88d-4138-ced7-884e4a2016f8",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(sess.run(tf.shape(D)))\n",
        "print(sess.run(D))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 4 4 1]\n",
            "[[[[3.]\n",
            "   [3.]\n",
            "   [3.]\n",
            "   [3.]]\n",
            "\n",
            "  [[6.]\n",
            "   [6.]\n",
            "   [6.]\n",
            "   [6.]]\n",
            "\n",
            "  [[6.]\n",
            "   [6.]\n",
            "   [6.]\n",
            "   [6.]]\n",
            "\n",
            "  [[3.]\n",
            "   [3.]\n",
            "   [3.]\n",
            "   [3.]]]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "kOydnrj_9np-",
        "colab_type": "code",
        "outputId": "6aa1da8f-0991-4a6d-8cf7-bd77cfb6e84e",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "M = np.array([[0,1,2],[2,3,4],[4,5,6]]).reshape([1,3,3,1])\n",
        "x = model.predict(M)\n",
        "print(M.reshape([3,3]))\n",
        "print(x.reshape([9,9]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0 1 2]\n",
            " [2 3 4]\n",
            " [4 5 6]]\n",
            "[[0. 0. 0. 0. 1. 2. 2. 2. 2.]\n",
            " [0. 0. 0. 0. 1. 2. 2. 2. 2.]\n",
            " [0. 0. 0. 0. 1. 2. 2. 2. 2.]\n",
            " [0. 0. 0. 0. 1. 2. 2. 2. 2.]\n",
            " [2. 2. 2. 2. 3. 4. 4. 4. 4.]\n",
            " [4. 4. 4. 4. 5. 6. 6. 6. 6.]\n",
            " [4. 4. 4. 4. 5. 6. 6. 6. 6.]\n",
            " [4. 4. 4. 4. 5. 6. 6. 6. 6.]\n",
            " [4. 4. 4. 4. 5. 6. 6. 6. 6.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "LSV4qHNo9nqC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}